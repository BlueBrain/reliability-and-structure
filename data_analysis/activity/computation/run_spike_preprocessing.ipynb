{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing pipeline: Spike preprocessing\n",
    "\n",
    "Loads extracted spike trains (in toposample format; from step 1), computes filtered spike signals (e.g., required for reliability) and firing rates, and stores them into a .h5 data store.\n",
    "\n",
    "__Preprocessing pipeline overview:__\n",
    "1. Extract & cut spike trains [`run_spike_extraction.ipynb`]\n",
    "2. Compute filtered spike signals [this notebook]\n",
    "3. Compute firing rates [this notebook]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../library')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "syn_class = 'EXC'  # 'EXC', 'INH', 'ALL'\n",
    "num_sims = 10  # Number of simulations (extracted spike files) in working_dir\n",
    "working_dir = '/path/to/working_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute filtered spike signals\n",
    "\n",
    "Runs preprocessing (filtering, w/o mean-centering) of (excitatory) spike trains [PARALLEL IMPLEMENTATION]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import run_preprocessing, merge_into_h5_data_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing in 194.994s\n"
     ]
    }
   ],
   "source": [
    "# Run preprocessing\n",
    "spike_file_names = [f'raw_spikes_{syn_class.lower()}_{idx}.npy' for idx in range(num_sims)]\n",
    "\n",
    "run_preprocessing(working_dir, spike_file_names, sigma=10.0, syn_class=syn_class, pool_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [22:24<00:00, 44.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 30 files merged into \"/gpfs/bbp.cscs.ch/data/scratch/proj9/bisimplices/bbp_workflow/ce776698-d3c9-468f-8714-92407570b292/working_dir_all/processed_data_store.h5\"\n"
     ]
    }
   ],
   "source": [
    "# Merge individual preprocessed spike files into .h5 data store\n",
    "#   split_by_gid==False ... Datasets per sim\n",
    "#   split_by_gid==True  ... Datasets per sim & GID\n",
    "tmp_file_names = [f'spike_signals_{syn_class.lower()}_{idx}__tmp__.npz' for idx in range(num_sims)]\n",
    "\n",
    "h5_file = merge_into_h5_data_store(working_dir, tmp_file_names, data_store_name='processed_data_store', split_by_gid=False, syn_class=syn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `OR` 2b. Create empty data store without filtered spike signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import create_empty_data_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_empty_data_store(working_dir, data_store_name='processed_data_store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compute firing rates\n",
    "\n",
    "Runs firing rate extraction based on mean inverse inter-spike interval of (excitatory) spike trains [PARALLEL IMPLEMENTATION]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import run_rate_extraction, merge_rates_to_h5_data_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run rate extraction\n",
    "spike_file_names = [f'raw_spikes_{syn_class.lower()}_{idx}.npy' for idx in range(num_sims)]\n",
    "\n",
    "run_rate_extraction(working_dir, spike_file_names, syn_class=syn_class, pool_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 564.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 30 files merged and added to \"/gpfs/bbp.cscs.ch/data/scratch/proj9/bisimplices/bbp_workflow/ce776698-d3c9-468f-8714-92407570b292/working_dir_all/processed_data_store.h5\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Merge individual rate files into .h5 data store\n",
    "tmp_file_names = [f'firing_rates_{syn_class.lower()}_{idx}__tmp__.npz' for idx in range(num_sims)]\n",
    "\n",
    "h5_file = merge_rates_to_h5_data_store(working_dir, tmp_file_names, data_store_name='processed_data_store', do_overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__HOW TO LOAD PROCESSED SPIKE SIGNALS, META-INFO, AND RATES FROM .H5 DATA STORE:__\n",
    "~~~\n",
    "h5_store = h5py.File(h5_file, 'r')\n",
    "print(f'Groups/Datasets: {list(h5_store.keys())}')\n",
    "\n",
    "t_bins = np.array(h5_store['t_bins'])\n",
    "gids = np.array(h5_store['gids'])\n",
    "firing_rates = np.array(h5_store['firing_rates'])\n",
    "sigma = np.array(h5_store['sigma']).tolist()\n",
    "if 'mean_centered' in h5_store:  # [Backward compatibility]\n",
    "    mean_centered = np.array(h5_store['mean_centered']).tolist()\n",
    "else:\n",
    "    mean_centered = False\n",
    "\n",
    "print(f'Spike signals per sims: {list(h5_store[\"spike_signals_exc\"].keys())}')\n",
    "if split_by_gid == True:\n",
    "    print(f'Spike signals within sim <SIM_IDX>: {list(h5_store[\"spike_signals_exc/sim_<SIM_IDX>\"].keys())}')\n",
    "    spike_signal = np.array(h5_store[f'spike_signals_exc/sim_{<SIM_IDX>}/gid_{<GID>}'])\n",
    "else:\n",
    "    spike_signals = np.array(h5_store[f'spike_signals_exc/sim_{<SIM_IDX>}'])\n",
    "\n",
    "h5_store.close()\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BbpWorkflowKernel",
   "language": "python",
   "name": "bbpworkflowkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
